{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from kfp.dsl import pipeline\n",
    "from kfp.dsl import component\n",
    "from kfp.dsl import OutputPath\n",
    "\n",
    "from kfp.dsl import (\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Input,\n",
    "    Model,\n",
    "    Output,\n",
    "    Metrics,\n",
    "    component,\n",
    "    Markdown,\n",
    "    HTML,\n",
    ")\n",
    "\n",
    "from kfp import compiler\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "import json\n",
    "\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "PROJECT_ID = config[\"PROJECT_ID\"]\n",
    "PIPELINE_ROOT = config[\"PIPELINE_ROOT\"]\n",
    "LOCATION = config[\"LOCATION\"]\n",
    "SERVICE_ACCOUNT = config[\"SERVICE_ACCOUNT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    staging_bucket=PIPELINE_ROOT,\n",
    "    location=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.2\",\n",
    "        \"google-cloud-aiplatform==1.49.0\",\n",
    "        \"google-cloud-bigquery==3.15.0\",\n",
    "        \"pyarrow==12.0.1\",\n",
    "        \"db-dtypes==1.1.1\",\n",
    "    ],\n",
    "    base_image=\"python:3.10.6\",\n",
    ")\n",
    "def download_data(table_id: str, credentials: dict, dataset: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    credentials_info = json.loads(json.dumps(credentials))\n",
    "    with open(\"credentials.json\", \"w\") as f:\n",
    "        json.dump(credentials_info, f)\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./credentials.json\"\n",
    "\n",
    "    client = bigquery.Client(location=\"EU\")\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM `{table_id}`\n",
    "    WHERE \n",
    "     DATETIME_DIFF( CURRENT_TIMESTAMP(), modified_date_time, MINUTE) < 30\n",
    "    \"\"\"\n",
    "    df = client.query(query).to_dataframe()\n",
    "\n",
    "    df.to_csv(dataset.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.2\",\n",
    "        \"requests==2.28.1\",\n",
    "        \"Pillow==9.2.0\",\n",
    "        \"vertexai==1.49.0\",\n",
    "        \"pyarrow==12.0.1\",\n",
    "    ],\n",
    "    base_image=\"python:3.10.6\",\n",
    ")\n",
    "def process_data(\n",
    "    input_data: Input[Dataset],\n",
    "    output_data: Output[Dataset],\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "):\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    import PIL\n",
    "    from vertexai.vision_models import Image, MultiModalEmbeddingModel\n",
    "    import vertexai\n",
    "\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "\n",
    "    model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding\")\n",
    "\n",
    "    df = pd.read_csv(input_data.path)\n",
    "\n",
    "    df[\"image_embedding\"] = [list() for _ in range(df.shape[0])]\n",
    "    df[\"text_embedding\"] = [list() for _ in range(df.shape[0])]\n",
    "\n",
    "    image_embedding_column = df.columns.get_loc(\"image_embedding\")\n",
    "    text_embedding_column = df.columns.get_loc(\"text_embedding\")\n",
    "\n",
    "    def download_image(url, filename):\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "        else:\n",
    "            print(f\"Error downloading image {url}\")\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        print(i)\n",
    "        download_image(df.image_url[i], \"/tmp/thumbnail.jpg\")\n",
    "\n",
    "        img = PIL.Image.open(\"/tmp/thumbnail.jpg\")\n",
    "        if img.format == \"WEBP\":\n",
    "            img = img.convert(\"RGB\")\n",
    "            img.save(\"/tmp/thumbnail.jpg\", \"JPEG\")\n",
    "\n",
    "        image = Image.load_from_file(\"/tmp/thumbnail.jpg\")\n",
    "        embeddings = model.get_embeddings(\n",
    "            image=image,\n",
    "            contextual_text=df.description[i],\n",
    "        )\n",
    "        df.iat[i, image_embedding_column] = embeddings.image_embedding\n",
    "        df.iat[i, text_embedding_column] = embeddings.text_embedding\n",
    "\n",
    "    df.to_parquet(output_data.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update BQ Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.2\",\n",
    "        \"google-cloud-bigquery==3.15.0\",\n",
    "        \"db-dtypes==1.1.1\",\n",
    "        \"pyarrow==12.0.1\",\n",
    "    ],\n",
    "    base_image=\"python:3.10.6\",\n",
    ")\n",
    "def update_bq_table(input_data: Input[Dataset], credentials: dict):\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    credentials_info = json.loads(json.dumps(credentials))\n",
    "    with open(\"credentials.json\", \"w\") as f:\n",
    "        json.dump(credentials_info, f)\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./credentials.json\"\n",
    "\n",
    "    client = bigquery.Client(location=\"EU\")\n",
    "    df = pd.read_parquet(input_data.path)\n",
    "\n",
    "    query = \"\"\"SELECT * FROM `aicamp_2024.similar_table`\"\"\"\n",
    "    existing_table = client.query(query).to_dataframe()\n",
    "\n",
    "    df = pd.concat([df, existing_table], ignore_index=True)\n",
    "    df[\"modified_date_time\"] = pd.to_datetime(df[\"modified_date_time\"])\n",
    "    df[\"item_id\"] = df[\"item_id\"].astype(str)\n",
    "    df = df.sort_values(\"modified_date_time\", ascending=False).drop_duplicates(\n",
    "        \"item_id\", keep=\"first\"\n",
    "    )\n",
    "\n",
    "    def dataframe_to_bq_table(df, table_id, write_disposition):\n",
    "        job_config = bigquery.LoadJobConfig()\n",
    "        job_config.write_disposition = write_disposition\n",
    "        job_config.create_disposition = \"CREATE_IF_NEEDED\"\n",
    "\n",
    "        job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n",
    "\n",
    "    dataframe_to_bq_table(df, \"aicamp_2024.similar_table\", \"WRITE_TRUNCATE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform==1.49.0\",\n",
    "        \"google-cloud-bigquery==3.15.0\",\n",
    "        \"db-dtypes==1.1.1\",\n",
    "        \"pyarrow==12.0.1\",\n",
    "        \"tabulate\",\n",
    "    ],\n",
    "    base_image=\"python:3.10.6\",\n",
    ")\n",
    "def get_similar_items(\n",
    "    credentials: dict,\n",
    "    output_sample_1: Output[Markdown],\n",
    "    output_sample_2: Output[Markdown],\n",
    "    output_sample_3: Output[Markdown],\n",
    "):\n",
    "    from google.cloud import bigquery\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    credentials_info = json.loads(json.dumps(credentials))\n",
    "    with open(\"credentials.json\", \"w\") as f:\n",
    "        json.dump(credentials_info, f)\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./credentials.json\"\n",
    "\n",
    "    client = bigquery.Client(location=\"EU\")\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            query.item_id, \n",
    "            base.item_id as similar_item_id,\n",
    "            distance, \n",
    "            query.image_url as query_image_url,\n",
    "            base.image_url as base_image_url,\n",
    "        FROM\n",
    "            VECTOR_SEARCH(\n",
    "                TABLE `aicamp_2024.similar_table`, \n",
    "                'image_embedding',\n",
    "                TABLE  `aicamp_2024.similar_table`, \n",
    "                query_column_to_search => 'image_embedding',\n",
    "                top_k => 10, distance_type => 'COSINE'\n",
    "            )\n",
    "        WHERE query.item_id != base.item_id\n",
    "        ORDER BY distance \n",
    "    \"\"\"\n",
    "    df = client.query(query).to_dataframe()\n",
    "\n",
    "    print(df.head())\n",
    "\n",
    "    sample_item_ids = df[df.distance < 0.2].item_id.sample(3)\n",
    "\n",
    "    def write_sample(output_sample, s):\n",
    "        with open(output_sample.path, \"w\") as f:\n",
    "            f.write(s)\n",
    "\n",
    "    for i in range(3):\n",
    "        item_id = sample_item_ids.iloc[i]\n",
    "        sample = df[df.item_id == item_id].copy()\n",
    "        sample[\"query_image_url\"] = sample[\"query_image_url\"].apply(\n",
    "            lambda x: f\"<img src='{x}' width='100'>\"\n",
    "        )\n",
    "        sample[\"base_image_url\"] = sample[\"base_image_url\"].apply(\n",
    "            lambda x: f\"<img src='{x}' width='100'>\"\n",
    "        )\n",
    "        output_sample = sample.to_markdown(index=False)\n",
    "        if i == 0:\n",
    "            write_sample(output_sample_1, output_sample)\n",
    "        elif i == 1:\n",
    "            write_sample(output_sample_2, output_sample)\n",
    "        else:\n",
    "            write_sample(output_sample_3, output_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name=\"similar_pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT + \"similar_pipeline\",\n",
    ")\n",
    "def similar_pipeline():\n",
    "\n",
    "    with open(\"service_account.json\", \"r\") as f:\n",
    "        raw_credential = json.load(f)\n",
    "\n",
    "    download_data_job = (\n",
    "        download_data(table_id=\"aicamp_2024.similar_view\", credentials=raw_credential)\n",
    "        .set_cpu_limit(\"2\")\n",
    "        .set_memory_limit(\"8G\")\n",
    "    )\n",
    "    print(download_data_job.outputs)\n",
    "\n",
    "    process_data_job = (\n",
    "        process_data(\n",
    "            input_data=download_data_job.outputs[\"dataset\"],\n",
    "            project_id=PROJECT_ID,\n",
    "            location=LOCATION,\n",
    "        )\n",
    "        .set_cpu_limit(\"2\")\n",
    "        .set_memory_limit(\"8G\")\n",
    "    )\n",
    "    print(process_data_job.outputs)\n",
    "\n",
    "    update_bq_table_job = (\n",
    "        update_bq_table(\n",
    "            input_data=process_data_job.outputs[\"output_data\"],\n",
    "            credentials=raw_credential,\n",
    "        )\n",
    "        .set_cpu_limit(\"2\")\n",
    "        .set_memory_limit(\"16G\")\n",
    "    )\n",
    "    print(update_bq_table_job.outputs)\n",
    "\n",
    "    get_similar_items_job = (\n",
    "        get_similar_items(credentials=raw_credential)\n",
    "        .after(update_bq_table_job)\n",
    "        .set_cpu_limit(\"2\")\n",
    "        .set_memory_limit(\"16G\")\n",
    "    )\n",
    "    print(get_similar_items_job.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=similar_pipeline, package_path=\"pipelines/similar_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"testing_similar_pipeline\",\n",
    "    template_path=\"pipelines/similar_pipeline.json\",\n",
    ")\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
