{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q vertexai==1.49.0 --upgrade\n",
    "# !pip install -q google-cloud-pipeline-components==2.6.0 --upgrade\n",
    "# !pip install -q kfp==2.4.0 --upgrade\n",
    "# !pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "\n",
    "from kfp.dsl import pipeline\n",
    "from kfp.dsl import component\n",
    "from kfp.dsl import OutputPath\n",
    "\n",
    "from kfp.v2.dsl import (\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Input,\n",
    "    Model,\n",
    "    Output,\n",
    "    Metrics,\n",
    "    component,\n",
    "    Markdown,\n",
    "    HTML,\n",
    ")\n",
    "\n",
    "from kfp import compiler\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "import json\n",
    "\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "PROJECT_ID = config[\"PROJECT_ID\"]\n",
    "PIPELINE_ROOT = config[\"PIPELINE_ROOT\"]\n",
    "LOCATION = config[\"LOCATION\"]\n",
    "SERVICE_ACCOUNT = config[\"SERVICE_ACCOUNT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    staging_bucket=PIPELINE_ROOT,\n",
    "    location=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.2\",\n",
    "        \"google-cloud-aiplatform==1.49.0\",\n",
    "        \"google-cloud-bigquery==3.15.0\",\n",
    "        \"pyarrow==12.0.1\",\n",
    "        \"db-dtypes==1.1.1\",\n",
    "    ],\n",
    "    base_image=\"python:3.10.6\",\n",
    ")\n",
    "def download_data(table_id: str, credentials: dict, dataset: Output[Dataset]):\n",
    "    \"\"\"\n",
    "    Downloads data from a BigQuery table and saves it as a CSV file.\n",
    "\n",
    "    Args:\n",
    "        table_id (str): The ID of the BigQuery table to download data from.\n",
    "        credentials (dict): A dictionary containing the credentials information.\n",
    "        dataset (Output[Dataset]): The output dataset where the CSV file will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    credentials_info = json.loads(json.dumps(credentials))\n",
    "    with open(\"credentials.json\", \"w\") as f:\n",
    "        json.dump(credentials_info, f)\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./credentials.json\"\n",
    "\n",
    "    client = bigquery.Client(location=\"EU\")\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM `{table_id}`\n",
    "    \"\"\"\n",
    "    df = client.query(query).to_dataframe()\n",
    "\n",
    "    df.to_csv(dataset.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.2\",\n",
    "        \"pyarrow==12.0.1\",\n",
    "    ],\n",
    "    base_image=\"python:3.10.6\",\n",
    ")\n",
    "def preprocess_data(input_data: Input[Dataset], output_data: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_csv(input_data.path)\n",
    "\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "\n",
    "    df[\"rank\"] = df.groupby(\"timestamp\")[\"page_views\"].rank(\n",
    "        ascending=False, method=\"first\"\n",
    "    )\n",
    "    df[\"next_timestamp_rank\"] = (\n",
    "        df.sort_values(\"timestamp\").groupby(\"item_id\")[\"rank\"].shift(-1)\n",
    "    )\n",
    "    df = df.sort_values(\"timestamp\")\n",
    "\n",
    "    def lag_features(df, lag_hours):\n",
    "        lagged_df = df.reset_index().copy()\n",
    "        lagged_df[\"timestamp\"] = lagged_df[\"timestamp\"] + pd.Timedelta(hours=lag_hours)\n",
    "        lagged_df = lagged_df.set_index([\"item_id\", \"timestamp\"])\n",
    "        lagged_df = lagged_df[[\"page_views\", \"impressions\", \"clicks\"]]\n",
    "        lagged_df.columns = [f\"{col}_lag_{lag_hours}\" for col in lagged_df.columns]\n",
    "        return lagged_df\n",
    "\n",
    "    df = df.set_index([\"item_id\", \"timestamp\"])\n",
    "\n",
    "    df = df.join(lag_features(df, 4), how=\"left\")\n",
    "    df = df.join(lag_features(df, 8), how=\"left\")\n",
    "    df = df.join(lag_features(df, 12), how=\"left\")\n",
    "    df = df.join(lag_features(df, 16), how=\"left\")\n",
    "\n",
    "    df = df.fillna(0)\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # df.to_csv(output_data.path, index=False)\n",
    "    df.to_parquet(output_data.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.2\",\n",
    "        \"pyarrow==12.0.1\",\n",
    "    ],\n",
    "    base_image=\"python:3.10.6\",\n",
    ")\n",
    "def train_test_split(\n",
    "    input_data: Input[Dataset], train_data: Output[Dataset], test_data: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits the input dataset into train and test datasets based on timestamps.\n",
    "\n",
    "    Args:\n",
    "        input_data (Input[Dataset]): The input dataset to be split.\n",
    "        train_data (Output[Dataset]): The output train dataset.\n",
    "        test_data (Output[Dataset]): The output test dataset.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.read_parquet(input_data.path)\n",
    "\n",
    "    target_col = \"next_timestamp_rank\"\n",
    "    baseline_target_col = \"rank\"\n",
    "\n",
    "    df = df[\n",
    "        df[target_col] < 30\n",
    "    ]  # remove items that are not in the top 30, for NDCG to work in XGBRanker\n",
    "    df = df[df[target_col] != 0]\n",
    "\n",
    "    # invert the ranks\n",
    "    df[target_col] = 31 - df[target_col]\n",
    "    df[baseline_target_col] = 31 - df[baseline_target_col]\n",
    "\n",
    "    timestamp_array = df.reset_index().sort_values(\"timestamp\")[\"timestamp\"].unique()\n",
    "    train_test_split_loc = int(len(timestamp_array) * 0.8)\n",
    "    train_timestamps = timestamp_array[:train_test_split_loc]\n",
    "    test_timestamps = timestamp_array[train_test_split_loc:]\n",
    "\n",
    "    train_df = df[df[\"timestamp\"].isin(train_timestamps)]\n",
    "    test_df = df[df[\"timestamp\"].isin(test_timestamps)]\n",
    "\n",
    "    train_df.to_parquet(train_data.path, index=False)\n",
    "    test_df.to_parquet(test_data.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.2\",\n",
    "        \"xgboost==2.0.1\",\n",
    "        \"scikit-learn==1.3.0\",\n",
    "        \"pyarrow==12.0.1\",\n",
    "    ],\n",
    "    base_image=\"python:3.10.6\",\n",
    ")\n",
    "def train_model(\n",
    "    train_data: Input[Dataset],\n",
    "    baseline_train_metric: Output[Metrics],\n",
    "    train_metrics: Output[Metrics],\n",
    "    # test_metrics: Output[Metrics],\n",
    "    model: Output[Model],\n",
    "    # sample_output: Output[HTML],\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a ranking model using XGBoost on the given training data.\n",
    "\n",
    "    Args:\n",
    "        train_data (Input[Dataset]): The input training dataset.\n",
    "        baseline_train_metric (Output[Metrics]): The output metrics for the baseline model.\n",
    "        train_metrics (Output[Metrics]): The output metrics for the trained model.\n",
    "        model (Output[Model]): The output trained model.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import ndcg_score\n",
    "\n",
    "    df = pd.read_parquet(train_data.path)\n",
    "\n",
    "    feature_cols = [\n",
    "        \"page_views\",\n",
    "        \"impressions\",\n",
    "        \"clicks\",\n",
    "        \"rank\",\n",
    "        \"page_views_lag_4\",\n",
    "        \"impressions_lag_4\",\n",
    "        \"clicks_lag_4\",\n",
    "        \"page_views_lag_8\",\n",
    "        \"impressions_lag_8\",\n",
    "        \"clicks_lag_8\",\n",
    "        \"page_views_lag_12\",\n",
    "        \"impressions_lag_12\",\n",
    "        \"clicks_lag_12\",\n",
    "        \"page_views_lag_16\",\n",
    "        \"impressions_lag_16\",\n",
    "        \"clicks_lag_16\",\n",
    "    ]\n",
    "    target_col = \"next_timestamp_rank\"\n",
    "    baseline_target_col = \"rank\"\n",
    "\n",
    "    dtrain = xgb.DMatrix(df[feature_cols], label=df[target_col])\n",
    "\n",
    "    # Set parameters\n",
    "    params = {\n",
    "        \"objective\": \"rank:ndcg\",  # Ranking objective\n",
    "        \"eval_metric\": \"ndcg@10\",  # Normalized Discounted Cumulative Gain at 10\n",
    "        \"max_depth\": 5,\n",
    "        \"eta\": 0.01,  # Learning rate\n",
    "        \"seed\": 42,  # Random seed for reproducibility\n",
    "    }\n",
    "\n",
    "    # Train the model\n",
    "    num_rounds = 1000  # Number of boosting rounds\n",
    "    ranker = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_rounds,\n",
    "        evals=[(dtrain, \"train\")],\n",
    "        verbose_eval=10,\n",
    "    )\n",
    "\n",
    "    y_train_pred = ranker.predict(dtrain)\n",
    "\n",
    "    baseline_train_metric.log_metric(\n",
    "        \"ndcg\", ndcg_score([df[target_col]], [df[baseline_target_col]], k=10)\n",
    "    )\n",
    "    train_metrics.log_metric(\"ndcg\", ndcg_score([df[target_col]], [y_train_pred], k=10))\n",
    "\n",
    "    ranker.save_model(model.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.2\",\n",
    "        \"pyarrow==12.0.1\",\n",
    "        \"xgboost==2.0.1\",\n",
    "        \"scikit-learn==1.3.0\",\n",
    "        \"google-cloud-aiplatform==1.49.0\",\n",
    "        \"tabulate\",  # for Markdown\n",
    "    ],\n",
    "    base_image=\"python:3.10.6\",\n",
    ")\n",
    "def test_model(\n",
    "    test_data: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    baseline_test_metric: Output[Metrics],\n",
    "    test_metrics: Output[Metrics],\n",
    "    sample_output: Output[Markdown],\n",
    "):\n",
    "    \"\"\"\n",
    "    Test the trained model using the provided test data.\n",
    "\n",
    "    Args:\n",
    "        test_data (Input[Dataset]): Input dataset containing the test data.\n",
    "        model (Input[Model]): Input model to be tested.\n",
    "        baseline_test_metric (Output[Metrics]): Output metrics for the baseline test.\n",
    "        test_metrics (Output[Metrics]): Output metrics for the test.\n",
    "        sample_output (Output[Markdown]): Output markdown file containing the sample predictions.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import ndcg_score\n",
    "\n",
    "    df = pd.read_parquet(test_data.path)\n",
    "\n",
    "    feature_cols = [\n",
    "        \"page_views\",\n",
    "        \"impressions\",\n",
    "        \"clicks\",\n",
    "        \"rank\",\n",
    "        \"page_views_lag_4\",\n",
    "        \"impressions_lag_4\",\n",
    "        \"clicks_lag_4\",\n",
    "        \"page_views_lag_8\",\n",
    "        \"impressions_lag_8\",\n",
    "        \"clicks_lag_8\",\n",
    "        \"page_views_lag_12\",\n",
    "        \"impressions_lag_12\",\n",
    "        \"clicks_lag_12\",\n",
    "        \"page_views_lag_16\",\n",
    "        \"impressions_lag_16\",\n",
    "        \"clicks_lag_16\",\n",
    "    ]\n",
    "    target_col = \"next_timestamp_rank\"\n",
    "    baseline_target_col = \"rank\"\n",
    "\n",
    "    dtest = xgb.DMatrix(df[feature_cols], label=df[target_col])\n",
    "\n",
    "    ranker = xgb.Booster()\n",
    "    ranker.load_model(model.path)\n",
    "\n",
    "    y_test_pred = ranker.predict(dtest)\n",
    "\n",
    "    baseline_test_metric.log_metric(\n",
    "        \"ndcg\", ndcg_score([df[target_col]], [df[baseline_target_col]], k=10)\n",
    "    )\n",
    "    test_metrics.log_metric(\"ndcg\", ndcg_score([df[target_col]], [y_test_pred], k=10))\n",
    "\n",
    "    df_sample = df[df.timestamp == df.timestamp.unique()[-1]].set_index(\"item_id\")\n",
    "    X_sample = df_sample[feature_cols]\n",
    "    # y_sample = df_sample[target_col]\n",
    "    d_sample = xgb.DMatrix(X_sample)\n",
    "    y_sample_pred = ranker.predict(d_sample)\n",
    "\n",
    "    y_sample_pred_rank = (\n",
    "        pd.DataFrame(\n",
    "            pd.Series(y_sample_pred, index=X_sample.index).rank(\n",
    "                ascending=True, method=\"first\"\n",
    "            )\n",
    "        )\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"rank\"})\n",
    "    )\n",
    "    y_sample_pred_rank = y_sample_pred_rank.sort_values(\"rank\")\n",
    "    y_sample_pred_rank[\"timestamp\"] = df_sample[\"timestamp\"].values[0]\n",
    "    with open(sample_output.path, \"w\") as f:\n",
    "        f.write(y_sample_pred_rank.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform==1.3.0\"],\n",
    "    base_image=\"python:3.10.6\",\n",
    ")\n",
    "def upload_model(\n",
    "    model: Input[Model],\n",
    "    project: str,\n",
    "    region: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Uploads a model to Google Cloud AI Platform.\n",
    "\n",
    "    Args:\n",
    "        model (Input[Model]): The model to upload.\n",
    "        project (str): The Google Cloud project ID.\n",
    "        region (str): The region where the model will be deployed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project, location=region)\n",
    "\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    logging.debug(model)\n",
    "\n",
    "    print(model)\n",
    "    print(model.uri)\n",
    "\n",
    "    import os\n",
    "\n",
    "    path, file = os.path.split(model.uri)\n",
    "\n",
    "    uploaded_model = aiplatform.Model.upload(\n",
    "        display_name=\"trending-content-ranker\",\n",
    "        artifact_uri=path,\n",
    "        serving_container_image_uri=f\"europe-docker.pkg.dev/{project}/custom-repo/xgboost-image:tag1\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trending Pipeline Steps:\n",
    "\n",
    "1. **Download Data:**\n",
    "   - Fetch data from BigQuery table \"aicamp_2024.trending_training_view\".\n",
    "   - Convert data to CSV format.\n",
    "   - Store data in a Dataset artifact.\n",
    "2. **Preprocess Data:**\n",
    "   - Read data from Dataset artifact.\n",
    "   - Convert timestamp column to datetime format.\n",
    "   - Calculate rank and lagged features for each item based on timestamp and page views, impressions, and clicks.\n",
    "   - Fill missing values with 0.\n",
    "   - Store preprocessed data in a Dataset artifact.\n",
    "3. **Train-Test Split:**\n",
    "   - Read data from Dataset artifact.\n",
    "   - Filter data to only include items with next timestamp rank less than 30 and not equal to 0.\n",
    "   - Split data into training and testing sets based on timestamps.\n",
    "   - Store training and testing data in separate Dataset artifacts.\n",
    "4. **Train Model:**\n",
    "   - Read training data from Dataset artifact.\n",
    "   - Train an XGBoost model using NDCG as the objective function.\n",
    "   - Log training metrics (NDCG score).\n",
    "   - Save the trained model in a Model artifact.\n",
    "5. **Test Model:**\n",
    "   - Read testing data from Dataset artifact.\n",
    "   - Load trained model from Model artifact.\n",
    "   - Calculate and log testing metrics (NDCG score).\n",
    "   - Generate sample predictions and store them in Markdown format.\n",
    "6. **Upload Model:**\n",
    "   - Load trained model from Model artifact.\n",
    "   - Upload the model to Vertex AI with the specified display name, artifact URI, and serving container image URI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(\n",
    "    name=\"treding_training_pipeline\",\n",
    "    pipeline_root=PIPELINE_ROOT + \"treding_training_pipeline\",\n",
    ")\n",
    "def trending_pipeline():\n",
    "    \"\"\"\n",
    "    This function defines a pipeline for training a trending model.\n",
    "\n",
    "    The pipeline consists of the following steps:\n",
    "    1. Downloading data from a specified table using service account credentials.\n",
    "    2. Preprocessing the downloaded data.\n",
    "    3. Splitting the preprocessed data into train and test sets.\n",
    "    4. Training a model using the train set.\n",
    "    5. Testing the trained model using the test set.\n",
    "    6. Uploading the trained model to a specified project and region.\n",
    "\n",
    "    Each step in the pipeline has specified CPU and memory limits.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(\"service_account.json\", \"r\") as f:\n",
    "        raw_credential = json.load(f)\n",
    "\n",
    "    download_data_job = (\n",
    "        download_data(\n",
    "            table_id=\"aicamp_2024.trending_training_view\", credentials=raw_credential\n",
    "        )\n",
    "        .set_cpu_limit(\"2\")\n",
    "        .set_memory_limit(\"8G\")\n",
    "    )\n",
    "    print(download_data_job.outputs)\n",
    "\n",
    "    preprocess_data_job = (\n",
    "        preprocess_data(input_data=download_data_job.outputs[\"dataset\"])\n",
    "        .set_cpu_limit(\"4\")\n",
    "        .set_memory_limit(\"16G\")\n",
    "    )\n",
    "    print(preprocess_data_job.outputs)\n",
    "\n",
    "    train_test_split_job = (\n",
    "        train_test_split(input_data=preprocess_data_job.outputs[\"output_data\"])\n",
    "        .set_cpu_limit(\"4\")\n",
    "        .set_memory_limit(\"16G\")\n",
    "    )\n",
    "\n",
    "    train_model_job = (\n",
    "        train_model(train_data=train_test_split_job.outputs[\"train_data\"])\n",
    "        .set_cpu_limit(\"12\")\n",
    "        .set_memory_limit(\"32G\")\n",
    "    )\n",
    "    print(train_model_job.outputs)\n",
    "\n",
    "    test_model_job = (\n",
    "        test_model(\n",
    "            test_data=train_test_split_job.outputs[\"test_data\"],\n",
    "            model=train_model_job.outputs[\"model\"],\n",
    "        )\n",
    "        .set_cpu_limit(\"4\")\n",
    "        .set_memory_limit(\"16G\")\n",
    "    )\n",
    "    print(test_model_job.outputs)\n",
    "\n",
    "    upload_model_job = (\n",
    "        upload_model(\n",
    "            model=train_model_job.outputs[\"model\"],\n",
    "            project=PROJECT_ID,\n",
    "            region=LOCATION,\n",
    "        )\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_memory_limit(\"2G\")\n",
    "    )\n",
    "    print(upload_model_job.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=trending_pipeline,\n",
    "    package_path=\"pipelines/treding_training_pipeline.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compiler.Compiler():` This creates an instance of the Compiler class.\n",
    "\n",
    "`.compile():` This is a method of the Compiler class. It's used to compile a pipeline function into a file.\n",
    "\n",
    "`pipeline_func=trending_pipeline:` This is the pipeline function that you want to compile. trending_pipeline is a function that defines your pipeline.\n",
    "\n",
    "`package_path=\"pipelines/trending_training_pipeline.json\":` This is the path where the output JSON file will be written. The compiled pipeline will be saved in this file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"treding_training_pipeline\",\n",
    "    template_path=\"pipelines/treding_training_pipeline.json\",\n",
    "    enable_caching=True,\n",
    ")\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
